# =============================================
# ğŸ“˜ TXT â†’ Chroma ë²¡í„° DB ì €ì¥
# =============================================
import os
import re
import shutil
import unicodedata
import pandas as pd
from langchain_text_splitters import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma

# =============================================
# ğŸ“Œ ê²½ë¡œ ì„¤ì •
# =============================================
docs_folder = "./result_txt"       # txt_transfer.py ê²°ê³¼
metadata_file = "./result_files/metadata.xlsx"
db_path = "./chroma_db"             # DB ì €ì¥ ê²½ë¡œ

# ì´ì „ DB ì‚­ì œ
if os.path.exists(db_path):
    shutil.rmtree(db_path)
    print("ğŸ—‘ï¸ ê¸°ì¡´ DB ì‚­ì œ ì™„ë£Œ")
os.makedirs(db_path, exist_ok=True)
print(f"ğŸ“ ìƒˆ DB ìƒì„±: {db_path}")

# =============================================
# ğŸ“Œ í…ìŠ¤íŠ¸ ë¶„í• 
# =============================================
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100,
    separator="\n"
)

# =============================================
# ğŸ“Œ ì„ë² ë”© ëª¨ë¸
# =============================================
hf_embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# =============================================
# ğŸ“Œ Chroma DB ë¶ˆëŸ¬ì˜¤ê¸°
# =============================================
db = Chroma(persist_directory=db_path, embedding_function=hf_embeddings)

# =============================================
# ğŸ“Œ íŒŒì¼ëª… OS-safe í‚¤ ë³€í™˜ í•¨ìˆ˜
# =============================================
def safe_search_key(name):
    name = re.sub(r'[<>:"/\\|?*]', "_", name)
    name = name.replace(" ", "_")
    return name

# =============================================
# ğŸ“Œ ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# =============================================
metadata_df = pd.read_excel(metadata_file)
metadata_dict = {}
for _, row in metadata_df.iterrows():
    orig = unicodedata.normalize('NFC', str(row["ê²Œì‹œê¸€ ì œëª©"]).strip())
    meta = {
        "file_name": orig,
        "department": str(row.get("ê´€ë ¨ë¶€ì„œ", "") or "").strip(),
        "url": str(row.get("URL", "") or "").strip(),
        "date": str(row.get("ì‘ì„±ì¼", "") or "").strip()
    }
    metadata_dict[orig] = meta
    metadata_dict[safe_search_key(orig)] = meta

# =============================================
# ğŸ“Œ TXT íŒŒì¼ ë¡œë“œ ë° DB ì¶”ê°€
# =============================================
file_count = 0
for filename in os.listdir(docs_folder):
    if not filename.endswith(".txt"):
        continue

    base_name = unicodedata.normalize('NFC', os.path.splitext(filename)[0].strip())
    safe_key_name = safe_search_key(base_name)

    meta = metadata_dict.get(base_name) or metadata_dict.get(safe_key_name)
    if meta is None:
        print(f"âš ï¸ {filename} ë©”íƒ€ë°ì´í„° ì—†ìŒ â†’ ê±´ë„ˆëœ€")
        continue

    try:
        loader = TextLoader(os.path.join(docs_folder, filename), encoding="utf-8")
        documents = loader.load_and_split(text_splitter=text_splitter)
        for doc in documents:
            doc.metadata.update(meta)

        db.add_documents(documents)
        file_count += 1

        print(f"âœ… {filename} ì¶”ê°€ ì™„ë£Œ ({len(documents)}ê°œ ì²­í¬)")

    except Exception as e:
        print(f"âš ï¸ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

print(f"\nğŸ‰ ì´ {file_count}ê°œ txt ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥ ì™„ë£Œ!")
print(f"ğŸ“ DB ê²½ë¡œ: {db_path}")
